{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice's Adventures in Wonderland by Lewis Carroll \n",
      "['alice', 'adventures', 'in', 'wonderland', 'by', 'lewis', 'carroll', 'the', 'millennium', 'fulcrum', 'edition', '3', 'contents', 'chapter', 'i', 'down', 'the', 'rabbit', 'chapter', 'ii', 'the', 'pool', 'of', 'tears', 'chapter']\n",
      "corpus len:  25320\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "f = open('alice_in_wonderland.txt','r')\n",
    "while(1):\n",
    "    line =  f.readline()\n",
    "    if len(line) == 0: break\n",
    "    corpus.extend(line.split())\n",
    "        \n",
    "f.close()\n",
    "corpus = ' '.join(corpus)\n",
    "print(corpus[:50])\n",
    "\n",
    "def clean_word(word):\n",
    "    word = word.lower()\n",
    "    for punctuation in ['\"',\"'\",'.',',','-','?','!',';',':','â€”','(',')','[',']']:\n",
    "        word = word.split(punctuation)[0]\n",
    "    return word\n",
    "\n",
    "\n",
    "\n",
    "corpus = [clean_word(word) for word in corpus.split()]\n",
    "corpus = [word for word in corpus if len(word) > 0]\n",
    "print(corpus[:25])\n",
    "D = len(corpus)\n",
    "print('corpus len: ',D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word list size (number of distinct words):  2637\n"
     ]
    }
   ],
   "source": [
    "tokenize = {}\n",
    "wordlist = []\n",
    "token = 0\n",
    "for word in corpus:\n",
    "    if word not in tokenize.keys():\n",
    "        tokenize[word] = token\n",
    "        wordlist.append(word)\n",
    "        token += 1\n",
    "    \n",
    "V = len(wordlist)\n",
    "print('word list size (number of distinct words): ', V)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [9. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# bin how many times a word follows another word\n",
    "counts_2gram = np.zeros((V,V))\n",
    "for i in range(1,len(corpus)):\n",
    "    token_i = tokenize[corpus[i]]\n",
    "    token_im1 = tokenize[corpus[i-1]]\n",
    "    counts_2gram[token_i,token_im1] += 1\n",
    "\n",
    "print(counts_2gram)\n",
    "\n",
    "#first line of matrix is blank because first word has no previous word\n",
    "\n",
    "#2637 by 2637 matrix created as an 2d array of 2367 rows with each row of length 2637\n",
    "#each entry data is correlated with a word pair\n",
    "#for example, counts_2gram[1][0] represents the number of times 'adventure' appears after 'alice'\n",
    "#the rows are each word\n",
    "#row0 = alice\n",
    "#row2 = adventures\n",
    "#row3 = in\n",
    "#row4 = wonderland\n",
    "#column 0 = alice\n",
    "#column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('was', 0.0007109004739336494)\n",
      "('queen', 0.002764612954186414)\n",
      "('cat', 0.00019747235387045816)\n",
      "('turtle', 0.0022511848341232226)\n",
      "('and', 0.00015797788309636652)\n",
      "('said', 0.0001579778830963665)\n",
      "0.2500098740076622\n"
     ]
    }
   ],
   "source": [
    "#past word as feature\n",
    "\n",
    "posterior_1word = np.zeros((V, V))\n",
    "prior = np.zeros(V)\n",
    "\n",
    "def get_likelihood_2gram(word):\n",
    "    #get index of word in wordlist\n",
    "    \n",
    "    index = wordlist.index(word)    \n",
    "    #returned is an array of posteriors or P(x | y)'s , an array of values of #ocurrences of each word coming before \n",
    "    #the input Word\n",
    "    #It needs to be divided by the sum of all words because?\n",
    "    \n",
    "    posterior_1word = counts_2gram[:,index] / np.sum(counts_2gram, axis = 1)[index]\n",
    "    prior = np.sum(counts_2gram, axis = 1)[index] / len(corpus)\n",
    "    \n",
    "    likelihood = posterior_1word * prior\n",
    "    #likelihood = P(x | y) * P(y)\n",
    "    #posterior: P(x | y) = get_likelihood_2gram(word)\n",
    "    #prior: P(y) = np.sum(counts_2gram, axis = 1)[index] / len(corpus)\n",
    "    \n",
    "    return(likelihood)\n",
    "\n",
    "\n",
    "def pred_2gram(word):    \n",
    "    #likelihood = P(x | y) * P(y)\n",
    "    #posterior: P(x | y) = get_likelihood_2gram(word)\n",
    "    #prior: P(y) = np.sum(counts_2gram, axis = 1)[index] / len(corpus)\n",
    "    \n",
    "    #now have an array of a bunch of calculated likelihoods, and return the largest one because that is the most\n",
    "    #probabilistically likely\n",
    "    likelihood = get_likelihood_2gram(word)\n",
    "    i = np.argmax(likelihood)\n",
    "    return(wordlist[i], likelihood[i])\n",
    "\n",
    "\n",
    "print(pred_2gram('alice'))\n",
    "print(pred_2gram('the'))\n",
    "print(pred_2gram('cheshire'))\n",
    "print(pred_2gram('mock'))\n",
    "print(pred_2gram('cat'))\n",
    "print(pred_2gram('turtle'))\n",
    "\n",
    "def classification_accuracy():\n",
    "    totalCount = 0\n",
    "    accuracyCount = 0\n",
    "    for i in range(0, len(corpus)-1):\n",
    "        currentWord = corpus[i]\n",
    "        mostLikeyNextWord = pred_2gram(currentWord)[0]\n",
    "                \n",
    "        totalCount+=1\n",
    "        if mostLikeyNextWord == corpus[i+1]:\n",
    "            accuracyCount+=1\n",
    "            \n",
    "    return accuracyCount / totalCount\n",
    "\n",
    "print(classification_accuracy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 7.529411764705882)\n",
      "('you', 0.17647058823529413)\n",
      "('up', 0.23529411764705882)\n",
      "('well', 7.898894154818325e-05)\n",
      "('girl', 0.00011848341232227489)\n",
      "('miles', 3.9494470774091624e-05)\n",
      "n = 3:  0.550420665955682\n",
      "n = 5:  0.745684376851669\n",
      "n = 10:  0.9441722639273015\n",
      "['it', 'said', 'the', 'king', 'and', 'the', 'queen', 'of', 'the', 'queen', 'and', 'the', 'queen', 'of', 'the', 'queen', 'and', 'the', 'queen', 'of', 'the', 'queen', 'and', 'the', 'queen']\n",
      "['all', 'this', 'a', 'little', 'alice', 'in', 'the', 'little', 'of', 'the', 'right', 'said', 'the', 'queen', 'and', 'in', 'it', 'to', 'the', 'first', 'she', 'was', 'the', 'cat', 'and']\n"
     ]
    }
   ],
   "source": [
    "#past 2 words as features\n",
    "\n",
    "#initialize the tensor (3d structure) to hold multiple matrices (2d array)\n",
    "#D = len(corpus)\n",
    "#V = len(wordlist)\n",
    "tensor = []\n",
    "for i in range(1, 31):\n",
    "    insertArray = np.zeros((V, V))\n",
    "    for j in range(0, D-i):\n",
    "        #go through entire corpus and insert into the tensor:\n",
    "        #the tokenization of each word-pair serves as index\n",
    "        #increment value at that index once to indicate the word-pair\n",
    "        token_i = tokenize[corpus[i+j]]\n",
    "        token_im1 = tokenize[corpus[j]]\n",
    "        insertArray[token_i, token_im1] += 1\n",
    "    tensor.append(insertArray)  \n",
    "\n",
    "\n",
    "posterior_2words = np.zeros((V, V))\n",
    "posterior_2gram = np.vstack([posterior_1word,posterior_2words])    \n",
    "    \n",
    "    \n",
    "#functions to predict words based on the past 2 words\n",
    "#it is the same formula as before but accounting for past two now\n",
    "def get_likelihood_3gram(word2ago,word1ago):\n",
    "    index = wordlist.index(word)\n",
    "    indexAgo = wordlist.index(word1ago)\n",
    "    return counts_2gram[:,indexAgo] / np.sum(counts_2gram,axis=0)[index]\n",
    "\n",
    "def pred_3gram(word2ago,word1ago):\n",
    "    likelihood = get_likelihood_3gram(word2ago,word1ago)\n",
    "    i = np.argmax(likelihood)\n",
    "    return wordlist[i], likelihood[i]\n",
    "\n",
    "\n",
    "#functions to predict words based on the past \"n\" words\n",
    "def get_likelihood_ngram(word_list):\n",
    "    likelihood = tensor[0][:,tokenize[word_list[len(word_list)-1]]]    \n",
    "    for i in range(1, len(word_list)):\n",
    "        likelihood = likelihood * tensor[i][:,tokenize[word_list[len(word_list)-1-i]]]\n",
    "    likelihood = likelihood / D\n",
    "    return likelihood\n",
    "\n",
    "def pred_ngram(word_list):\n",
    "    likelihood = get_likelihood_ngram(word_list)\n",
    "    i = np.argmax(likelihood)\n",
    "    return(wordlist[i], likelihood[i])\n",
    "\n",
    "print(pred_3gram('pack','of'))\n",
    "print(pred_3gram('the','mad'))\n",
    "print(pred_3gram('she','jumped'))\n",
    "print(pred_ngram(['before', 'she', 'found', 'herself', 'falling', 'down', 'a', 'very', 'deep']))\n",
    "print(pred_ngram(['what', 'an', 'ignorant', 'little']))\n",
    "print(pred_ngram(['four','thousand']))\n",
    "\n",
    "def classification_accuracy_n(nWords):\n",
    "    totalCount = 0\n",
    "    accuracyCount = 0\n",
    "    for i in range(D-nWords):\n",
    "        mostLikeyNextWord = pred_ngram([corpus[i+j] for j in range(nWords)])[0]\n",
    "        if mostLikeyNextWord == corpus[i+nWords]:\n",
    "            accuracyCount+=1\n",
    "        totalCount+=1\n",
    "    return accuracyCount / totalCount\n",
    "\n",
    "print(\"n = 3: \", classification_accuracy_n(3))\n",
    "print(\"n = 5: \", classification_accuracy_n(5))\n",
    "print(\"n = 10: \", classification_accuracy_n(10))\n",
    "\n",
    "def textGenerationMostLikely(seedPhrase):\n",
    "    for i in range(25):\n",
    "        seedPhrase.append(pred_ngram(seedPhrase[-3:])[0])\n",
    "    return seedPhrase[3:]\n",
    "\n",
    "def textGenerationSamplingProbability(seedPhrase):\n",
    "    for i in range(25):\n",
    "        likelihood_arr = get_likelihood_ngram(seedPhrase[-3:])\n",
    "        #as stupid as this sounds, theres some failure rate with random.choices because of the way the weights\n",
    "        #are calculated, resulting in some weights of 0, which random.choices does not like\n",
    "        #however, all words need to be given a weight, so a way to bypass ValueError Exception is to run it\n",
    "        #again and again until the case occurs where no weights are 0.0\n",
    "        for x in range(10000):\n",
    "            try:\n",
    "                mostLikeyNextWord = random.choices(wordlist, weights=likelihood_arr)\n",
    "            except Exception as ValueError:\n",
    "                pass\n",
    "            else:\n",
    "                break\n",
    "        seedPhrase.append(mostLikeyNextWord[0])\n",
    "    return seedPhrase[3:]\n",
    "\n",
    "print(textGenerationMostLikely(['the', 'mad', 'hatter']))\n",
    "print(textGenerationSamplingProbability(['the', 'mad', 'hatter']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
