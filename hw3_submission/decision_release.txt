{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import scipy.stats as ss\n",
    "\n",
    "import sklearn.model_selection as ms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndata = []\\nf = open('covtype.data','r')\\nwhile(1):\\n    line = f.readline()\\n    if  len(line) < 100:\\n        print line\\n    \\n    if len(line) == 0: break\\n    data.append(np.array([float(k) for k in line.split(',')]))\\n    if len(data) % 100000 == 0:\\n        print len(data)\\n        \\nf.close\\ndata = np.vstack(data)\\nN = data.shape[0]\\nidx = np.random.permutation(N)\\n\\n\\nX_test = data[:N/5,:]\\nX_train = data[N/5:,:]\\ny_test = X_test[:,-1]\\ny_train = X_train[:,-1]\\nX_test = X_test[:,:-1]\\nX_train = X_train[:,:-1]\\n\\n\\nsio.savemat('covtype.mat',{'X_train':X_train,'X_test':X_test,'y_train':y_train,'y_test':y_test})\\n\\ndata = sio.loadmat('covtype.mat')\\nX_train = data['X_train']\\nX_test = data['X_test']\\ny_train = data['y_train'][0]\\ny_test = data['y_test'][0]\\n\\ny_idx_train = [np.where(np.equal(y_train,k))[0] for k in np.unique(y_train)]\\n\\nfor i in xrange(len(y_idx_train)):\\n    y_idx = y_idx_train[i]\\n    y_idx_train[i] = y_idx[np.random.choice(len(y_idx),len(y_idx)/1000+1,replace=False)]\\n    \\ny_idx_train = np.hstack(y_idx_train)\\ny_idx_train = np.random.permutation(y_idx_train)\\n\\nX_train = X_train[y_idx_train,:]\\ny_train = y_train[y_idx_train]\\n\\nsio.savemat('covtype_reduced.mat',{'X_train':X_train,'X_test':X_test,'y_train':y_train,'y_test':y_test})\\n\\n\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load your data (don't touch, just run)\n",
    "\"\"\"\n",
    "data = []\n",
    "f = open('covtype.data','r')\n",
    "while(1):\n",
    "    line = f.readline()\n",
    "    if  len(line) < 100:\n",
    "        print line\n",
    "    \n",
    "    if len(line) == 0: break\n",
    "    data.append(np.array([float(k) for k in line.split(',')]))\n",
    "    if len(data) % 100000 == 0:\n",
    "        print len(data)\n",
    "        \n",
    "f.close\n",
    "data = np.vstack(data)\n",
    "N = data.shape[0]\n",
    "idx = np.random.permutation(N)\n",
    "\n",
    "\n",
    "X_test = data[:N/5,:]\n",
    "X_train = data[N/5:,:]\n",
    "y_test = X_test[:,-1]\n",
    "y_train = X_train[:,-1]\n",
    "X_test = X_test[:,:-1]\n",
    "X_train = X_train[:,:-1]\n",
    "\n",
    "\n",
    "sio.savemat('covtype.mat',{'X_train':X_train,'X_test':X_test,'y_train':y_train,'y_test':y_test})\n",
    "\n",
    "data = sio.loadmat('covtype.mat')\n",
    "X_train = data['X_train']\n",
    "X_test = data['X_test']\n",
    "y_train = data['y_train'][0]\n",
    "y_test = data['y_test'][0]\n",
    "\n",
    "y_idx_train = [np.where(np.equal(y_train,k))[0] for k in np.unique(y_train)]\n",
    "\n",
    "for i in xrange(len(y_idx_train)):\n",
    "    y_idx = y_idx_train[i]\n",
    "    y_idx_train[i] = y_idx[np.random.choice(len(y_idx),len(y_idx)/1000+1,replace=False)]\n",
    "    \n",
    "y_idx_train = np.hstack(y_idx_train)\n",
    "y_idx_train = np.random.permutation(y_idx_train)\n",
    "\n",
    "X_train = X_train[y_idx_train,:]\n",
    "y_train = y_train[y_idx_train]\n",
    "\n",
    "sio.savemat('covtype_reduced.mat',{'X_train':X_train,'X_test':X_test,'y_train':y_train,'y_test':y_test})\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 3. 4. 5. 6. 7.] [1. 2. 3. 4. 5. 6. 7.]\n",
      "(468, 54) (116202, 54) (468,) (116202,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = sio.loadmat('covtype_reduced.mat')\n",
    "X_train = data['X_train']\n",
    "X_test = data['X_test']\n",
    "y_train = data['y_train'][0]\n",
    "y_test = data['y_test'][0]\n",
    "\n",
    "print(np.unique(y_train), np.unique(y_test))\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy =  3.3141823231610834\n",
      "conditional entropy =  3.3029598816135173\n"
     ]
    }
   ],
   "source": [
    "def entropy(label):\n",
    "    summation = 0\n",
    "    labelLength = len(label)\n",
    "    uniquesCount = {}\n",
    "    # get unique counts for frequency and probabilities of label\n",
    "    for x in label:\n",
    "        if x not in uniquesCount.keys():\n",
    "            uniquesCount[x] = 1\n",
    "        else:\n",
    "            uniquesCount[x] = uniquesCount[x]+1\n",
    "    # calculate the summation\n",
    "    for x in uniquesCount:\n",
    "        probability = uniquesCount[x] / labelLength\n",
    "        # case when log2(0) = 0, just ignore it\n",
    "        if probability != 0:\n",
    "            summation += probability * np.log2(probability)\n",
    "    return summation * -1.\n",
    "\n",
    "def cond_entropy(label,split):\n",
    "    summation = 0\n",
    "    labelLength = len(label)\n",
    "    splitLength = len(split)\n",
    "    \n",
    "    # get unique counts for frequency and probabilities of label\n",
    "    uniquesCountLabel = {}\n",
    "    for x in label:\n",
    "        if x not in uniquesCountLabel.keys():\n",
    "            uniquesCountLabel[x] = 1\n",
    "        else:\n",
    "            uniquesCountLabel[x] = uniquesCountLabel[x]+1\n",
    "            \n",
    "    # get unique counts for frequency and probabilities of split            \n",
    "    uniquesCountSplit = {}\n",
    "    for x in split:\n",
    "        if x not in uniquesCountSplit.keys():\n",
    "            uniquesCountSplit[x] = 1\n",
    "        else:\n",
    "            uniquesCountSplit[x] = uniquesCountSplit[x]+1\n",
    "            \n",
    "    # gets unique counts for each unique label,split pairs\n",
    "    conditionalProbDict = {}\n",
    "    for x in range(labelLength):\n",
    "        currentTuple = (label[x], split[x])\n",
    "        if currentTuple not in conditionalProbDict.keys():\n",
    "            conditionalProbDict[currentTuple] = 1\n",
    "        else:\n",
    "            conditionalProbDict[currentTuple] += 1\n",
    "    \n",
    "    \n",
    "    for x in uniquesCountLabel.keys():\n",
    "        for y in uniquesCountSplit.keys():\n",
    "            currentTuple = (x,y)\n",
    "            \n",
    "            if currentTuple not in conditionalProbDict.keys():\n",
    "                continue\n",
    "            \n",
    "            # jointProb = P(x, y) = probability of event x and event y occuring at the same time\n",
    "            # this is calculated by finding the frequency of each pair i.e. (6, 1)\n",
    "            # probability of both label 6 and split 1 occuring : the frequency of 6,1 tuple divided by #all tuples\n",
    "            jointProb = conditionalProbDict[currentTuple] / labelLength\n",
    "            \n",
    "            # condProb = P(x | y) = probability of event x occuring, given that probability y is given/already occured\n",
    "            # find all occurences of y: uniquesCountsSplit[y]\n",
    "            # find how many of those occurences of y also have current uniqueCountLabel\n",
    "            # probability of label 6 and split 1 given that the sample is all tuples with split 1\n",
    "            condProb = conditionalProbDict[currentTuple] / uniquesCountSplit[y]\n",
    "        \n",
    "            # just check if conditional probability is not 0, to rule out edge case of log2(0) = 0\n",
    "            if condProb != 0:\n",
    "                summation += jointProb * np.log2(condProb)\n",
    "            \n",
    "    return summation * -1.\n",
    "\n",
    "random_sequences = sio.loadmat('random_sequences.mat')\n",
    "\n",
    "s1 = random_sequences['s1'][0]\n",
    "s2 = random_sequences['s2'][0]\n",
    "\n",
    "print('entropy = ', entropy(s1))\n",
    "print('conditional entropy = ', cond_entropy(s1,s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(468, 54)\n",
      "(468,)\n",
      "[[2410.   67.   14. ...    0.    0.    0.]\n",
      " [2739.   90.   25. ...    0.    0.    0.]\n",
      " [3172.    9.    7. ...    0.    0.    0.]\n",
      " ...\n",
      " [2910.  117.   15. ...    0.    0.    0.]\n",
      " [3204.  240.   17. ...    0.    0.    0.]\n",
      " [2802.   42.    7. ...    0.    0.    0.]]\n",
      "[3. 5. 1. 6. 2. 1. 3. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 3. 2. 1.\n",
      " 2. 2. 1. 3. 2. 2. 1. 2. 2. 3. 1. 1. 2. 1. 1. 1. 1. 1. 1. 2. 1. 3. 1. 3.\n",
      " 1. 1. 1. 1. 1. 2. 2. 1. 1. 2. 2. 7. 2. 7. 2. 2. 1. 1. 1. 2. 2. 1. 2. 2.\n",
      " 1. 1. 2. 1. 2. 1. 2. 1. 2. 2. 1. 2. 5. 1. 2. 2. 2. 2. 1. 1. 2. 2. 2. 1.\n",
      " 2. 1. 2. 2. 6. 1. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 3. 2. 6. 6. 2. 7. 2.\n",
      " 5. 6. 1. 1. 1. 2. 1. 2. 2. 2. 2. 2. 2. 3. 2. 2. 6. 2. 1. 1. 1. 1. 5. 2.\n",
      " 2. 2. 2. 1. 2. 2. 2. 4. 1. 2. 1. 2. 2. 2. 1. 2. 2. 6. 3. 1. 3. 1. 1. 2.\n",
      " 2. 7. 2. 1. 3. 2. 6. 3. 1. 3. 2. 1. 2. 1. 1. 5. 1. 1. 7. 1. 2. 1. 2. 2.\n",
      " 2. 1. 2. 3. 1. 1. 7. 2. 1. 2. 3. 6. 2. 2. 1. 2. 6. 2. 6. 1. 7. 1. 1. 2.\n",
      " 2. 2. 2. 2. 1. 2. 2. 2. 1. 7. 2. 2. 1. 1. 1. 3. 1. 1. 2. 1. 1. 1. 1. 2.\n",
      " 1. 2. 2. 2. 2. 2. 1. 2. 2. 2. 5. 1. 7. 1. 2. 2. 1. 7. 2. 2. 2. 1. 2. 1.\n",
      " 3. 1. 1. 1. 1. 2. 2. 2. 6. 1. 2. 2. 1. 7. 5. 2. 1. 2. 1. 1. 1. 2. 2. 6.\n",
      " 1. 1. 3. 1. 1. 1. 2. 2. 2. 1. 2. 7. 1. 2. 2. 1. 1. 1. 2. 3. 1. 2. 2. 2.\n",
      " 2. 1. 1. 2. 1. 3. 2. 2. 7. 2. 1. 2. 1. 2. 1. 6. 2. 2. 1. 1. 1. 2. 1. 1.\n",
      " 1. 2. 2. 1. 1. 1. 1. 1. 7. 2. 1. 2. 3. 2. 2. 1. 2. 2. 2. 1. 6. 1. 1. 1.\n",
      " 2. 6. 1. 1. 2. 3. 1. 2. 1. 1. 2. 2. 1. 7. 2. 3. 2. 2. 2. 2. 2. 2. 1. 1.\n",
      " 2. 1. 1. 3. 2. 2. 1. 1. 7. 2. 2. 2. 1. 2. 1. 3. 1. 2. 2. 1. 1. 2. 3. 2.\n",
      " 3. 1. 2. 1. 1. 1. 1. 2. 2. 1. 3. 1. 2. 1. 1. 1. 2. 2. 1. 2. 1. 1. 2. 1.\n",
      " 2. 2. 2. 7. 1. 2. 3. 7. 2. 1. 2. 2. 2. 2. 7. 1. 3. 1. 2. 2. 1. 2. 1. 2.\n",
      " 1. 3. 2. 2. 1. 3. 1. 3. 2. 2. 2. 1.]\n",
      "information gained in first step 0.008707824728642288\n"
     ]
    }
   ],
   "source": [
    "def find_best_split(x,y):\n",
    "    best_feat = 0\n",
    "    splitval = 0.\n",
    "    set1 = range(int(len(y)/2))\n",
    "    set2 = range(int(len(y)/2),len(y))\n",
    "    \n",
    "    y_new = y_train * 0\n",
    "    y_new[set1] = 1\n",
    "    y_new1 = y_new[set1]\n",
    "    \n",
    "    y_new = y_train * 0\n",
    "    y_new[set2] = 1\n",
    "    y_new2 = y_new[set2]\n",
    "    \n",
    "    for feature in x\n",
    "    \n",
    "    return best_feat, splitval, set1, set2\n",
    "\n",
    "\n",
    "# X_train: there are 54 features that that result in one label\n",
    "# y_train: each value in y_train is the actual label to each '54 features'\n",
    "\n",
    "best_feat, splitval, set1, set2 = find_best_split(X_train, y_train)\n",
    "y_new = y_train * 0\n",
    "y_new[set1] = 1\n",
    "print('information gained in first step', entropy(y_train) - cond_entropy(y_train,y_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purity(y):\n",
    "    return ss.mode(y)[1]/len(y+0.)\n",
    "    \n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self,  sample_idx, nodeid,  is_leaf = True):\n",
    "        self.is_leaf = is_leaf\n",
    "        self.id = nodeid\n",
    "        self.sample_idx = sample_idx\n",
    "        self.children = []\n",
    "        \n",
    "        \n",
    "        \n",
    "    def visit_node(self, x):\n",
    "        if self.is_leaf:\n",
    "            return self.label\n",
    "        \"\"\" Fill me in \"\"\"\n",
    "        return self.children[0].visit_node(x)\n",
    "        \n",
    "    def add_split_details(self, splitfeat, splitval)  :\n",
    "        self.splitfeat = splitfeat\n",
    "        self.splitval = splitval\n",
    "    \n",
    "        \n",
    "class Tree:\n",
    "    def __init__(self, x,y):\n",
    "        m = len(y)\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.maxid = -1\n",
    "        self.root = self.construct_node(np.array(range(m)))\n",
    "        self.leaves = [self.root]\n",
    "        \n",
    "    def print_tree(self):\n",
    "        print('printing tree...')\n",
    "        def print_node(parent, node):\n",
    "            print(node.id, end='')\n",
    "            \n",
    "            if parent is not None:\n",
    "                print(', parent ', parent.id,end='')\n",
    "            else:\n",
    "                print(', ROOT',end='')\n",
    "                \n",
    "            print(', label ', node.label, end='')\n",
    "            if node.is_leaf: \n",
    "                print(', LEAF, ', 'nsamples %d, purity %.2f' %(len(node.sample_idx), purity(self.y[node.sample_idx])))\n",
    "            else:\n",
    "                print(', NONLEAF, split %d, val %.2f' % (node.splitfeat, node.splitval))\n",
    "            if not node.is_leaf:\n",
    "                for ch in node.children:\n",
    "                    print_node(node, ch)\n",
    "        print_node(None, self.root)\n",
    "        \n",
    "        \n",
    "    def construct_node(self, sample_idx):\n",
    "        node = Node(sample_idx, self.maxid + 1,  True)\n",
    "        node.label = 0 # fill me in\n",
    "        node.entropy = entropy(self.y[sample_idx])\n",
    "        node.num_mistakes = np.sum(np.not_equal(node.label, self.y[sample_idx]))\n",
    "        self.maxid += 1\n",
    "        return node\n",
    "        \n",
    "\n",
    "\n",
    "    def report_train_err(self):\n",
    "        total_mistakes = 0\n",
    "        for leaf in self.leaves:\n",
    "            total_mistakes += leaf.num_mistakes\n",
    "        return total_mistakes / (len(self.y)+0.)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def predict(self,x):\n",
    "        return self.root.visit_node(x)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_err(tree):\n",
    "    # get test error\n",
    "    num_test_mistakes = 0\n",
    "    for k in range(len(y_test)):\n",
    "        x,y = X_test[k,:],y_test[k]\n",
    "        if y != tree.predict(x):\n",
    "            num_test_mistakes += 1\n",
    "    return num_test_mistakes / (len(y_test)+0.)\n",
    "\n",
    "\n",
    "\n",
    "tree = Tree(X_train,y_train)\n",
    "tree.print_tree()\n",
    "print('current train err:', tree.report_train_err())\n",
    "print('current test err:', get_test_err(tree))\n",
    "\n",
    "\n",
    "# my first split\n",
    "best_feat, splitval, set1, set2 = find_best_split(X_train, y_train)\n",
    " \n",
    "left_child = tree.construct_node(set1)\n",
    "right_child = tree.construct_node(set2)\n",
    "tree.root.is_leaf = False\n",
    "tree.leaves.pop(tree.leaves.index(tree.root))\n",
    "tree.root.add_split_details(splitfeat = best_feat, splitval = splitval)\n",
    "\n",
    "\n",
    "tree.root.children = [left_child, right_child]\n",
    "tree.leaves.extend(tree.root.children)\n",
    "tree.print_tree()\n",
    "print('one step train err:', tree.report_train_err())\n",
    "print('one step test err:', get_test_err(tree))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
